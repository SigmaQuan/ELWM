# External Long-term and Working Memory argumented Recurrent Neural Networks (ELWM-RNNs)

Experiment on External Long-term and Working Memory argumented Recurrent Neural Networks (ELWM-RNNs).


## Prerequisties
- Python 2.7
- [Theano](https://github.com/Theano/Theano)
- [Keras](https://github.com/fchollet/keras)



## Experiment on algorithm learning
**Copy**
![alt_tag](unit_test/image/figure_4.png)
**NTM Memory Use During the Copy Task**
![alt_tag](unit_test/image/figure_6.png)

**Repeat Copy**
(in progress)

**Associative Recall**
(in progress)

**Dynamic N-Grams**
(in progress)

**Priority Sort**
(in progress)



## Experiment on language modeling
**Text8**



## Experiment on question answering 
**bAbI**


## Usage
To train a copy task:
```
    $ python main.py
```


## Future works
- Training HEM-RNN to learning *repeat copy*.
- Training HEM-RNN to learning *associative recall*.
- Training HEM-RNN to learning *priority sort*.
- Using HEM-RNN for other natural language processing tasks such as neural language model.


## Author
Zhibin Quan / [@SigmaQuan](https://github.com/SigmaQuan)
